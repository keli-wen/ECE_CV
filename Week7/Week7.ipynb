{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c3d6b0a-937d-4833-89c8-9d2f97408a54",
   "metadata": {},
   "source": [
    "## 本周作业\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Github或者主页下载运行一个超分算法，获得结果试着训练一两个Epoch，给出超分结果\n",
    "</div>     \n",
    "\n",
    "选择模型：[EDSR-PyTorch](https://github.com/sanghyun-son/EDSR-PyTorch)\n",
    "<img src=\"https://camo.githubusercontent.com/4bc45421df57c3901ec5d21da412680df9b2d74fee7c297ab4e6764868e805fb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362b2d6f72616e67652e737667\" alt=\"python version\" data-canonical-src=\"https://img.shields.io/badge/python-3.6+-orange.svg\" style=\"max-width: 100%;\">\n",
    "![](https://img.shields.io/badge/homework-@wkl-yellow.svg?style=flat)\n",
    "\n",
    "\n",
    "```\n",
    "@InProceedings{Lim_2017_CVPR_Workshops,\n",
    "  author = {Lim, Bee and Son, Sanghyun and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},\n",
    "  title = {Enhanced Deep Residual Networks for Single Image Super-Resolution},\n",
    "  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},\n",
    "  month = {July},\n",
    "  year = {2017}\n",
    "}\n",
    "```\n",
    "\n",
    "我们使用的数据集是[DIV2K](http://www.vision.ee.ethz.ch/%7Etimofter/publications/Agustsson-CVPRW-2017.pdf)，下载[链接🔗](https://cv.snu.ac.kr/research/EDSR/DIV2K.tar)。\n",
    "\n",
    ">\n",
    ">\n",
    ">```bash\n",
    "># for train\n",
    ">python main.py --model EDSR --scale 2 --patch_size 96 --save edsr_baseline_x2 --reset \n",
    ">\n",
    "># for test\n",
    ">CUDA_VISIBLE_DEVICES=1 python main.py --data_test Demo --scale 4 --pre_train ../experiment/edsr_baseline_x2/model/model_latest.pt --test_only -->save_results\n",
    ">```\n",
    "\n",
    "下面截取一些 training 时的 **Log**\n",
    "\n",
    "---\n",
    ">```\n",
    ">[Epoch 3]\tLearning rate: 1.00e-4\n",
    ">[1600/16000]\t[L1: 4.4706]\t2.3+1.5s\n",
    ">[3200/16000]\t[L1: 4.4269]\t2.2+1.2s\n",
    ">[4800/16000]\t[L1: 4.4215]\t2.2+1.2s\n",
    ">[6400/16000]\t[L1: 4.3603]\t2.2+1.2s\n",
    ">[8000/16000]\t[L1: 4.3189]\t2.3+1.2s\n",
    ">[9600/16000]\t[L1: 4.3030]\t2.4+1.1s\n",
    ">[11200/16000]\t[L1: 4.2870]\t2.6+0.9s\n",
    ">[12800/16000]\t[L1: 4.2929]\t2.6+0.9s\n",
    ">[14400/16000]\t[L1: 4.2714]\t2.6+0.9s\n",
    ">[16000/16000]\t[L1: 4.2578]\t2.7+0.8s\n",
    ">\n",
    ">Evaluation:\n",
    ">[DIV2K x2]\tPSNR: 34.029 (Best: 34.029 @epoch 3)\n",
    ">Forward: 2.14s\n",
    ">\n",
    ">Saving...\n",
    ">Total: 2.56s\n",
    ">\n",
    ">[Epoch 4]\tLearning rate: 1.00e-4\n",
    ">[1600/16000]\t[L1: 4.1744]\t2.4+1.4s\n",
    ">[3200/16000]\t[L1: 4.1518]\t2.4+1.0s\n",
    ">[4800/16000]\t[L1: 4.1185]\t2.5+1.0s\n",
    ">[6400/16000]\t[L1: 4.1251]\t2.5+1.0s\n",
    ">[8000/16000]\t[L1: 4.1394]\t2.4+1.0s\n",
    ">[9600/16000]\t[L1: 4.1214]\t2.5+1.0s\n",
    ">[11200/16000]\t[L1: 4.1149]\t2.5+1.0s\n",
    ">[12800/16000]\t[L1: 4.1061]\t2.4+1.0s\n",
    ">[14400/16000]\t[L1: 4.1054]\t2.5+1.0s\n",
    ">[16000/16000]\t[L1: 4.0968]\t2.4+1.0s\n",
    ">\n",
    ">Evaluation:\n",
    ">[DIV2K x2]\tPSNR: 34.266 (Best: 34.266 @epoch 4)\n",
    ">Forward: 2.13s\n",
    ">\n",
    ">Saving...\n",
    ">Total: 2.43s\n",
    ">```\n",
    "---\n",
    ">```\n",
    ">[Epoch 133]\tLearning rate: 1.00e-4\n",
    ">[1600/16000]\t[L1: 3.5245]\t2.5+1.3s\n",
    ">[3200/16000]\t[L1: 3.4206]\t2.6+0.9s\n",
    ">[4800/16000]\t[L1: 3.3988]\t2.7+0.8s\n",
    ">[6400/16000]\t[L1: 3.3818]\t2.5+1.0s\n",
    ">[8000/16000]\t[L1: 3.3493]\t2.5+1.0s\n",
    ">[9600/16000]\t[L1: 3.3763]\t2.5+1.0s\n",
    ">[11200/16000]\t[L1: 3.3944]\t2.5+1.0s\n",
    ">[12800/16000]\t[L1: 3.3929]\t2.5+1.0s\n",
    ">[14400/16000]\t[L1: 3.3858]\t2.5+1.0s\n",
    ">[16000/16000]\t[L1: 3.3972]\t2.5+1.0s\n",
    ">\n",
    ">Evaluation:\n",
    ">[DIV2K x2]\tPSNR: 35.349 (Best: 35.444 @epoch 119)\n",
    ">Forward: 2.12s\n",
    ">\n",
    ">Saving...\n",
    ">Total: 2.36s\n",
    ">\n",
    ">[Epoch 134]\tLearning rate: 1.00e-4\n",
    ">[1600/16000]\t[L1: 3.3487]\t2.6+1.2s\n",
    ">[3200/16000]\t[L1: 3.4842]\t2.6+0.9s\n",
    ">[4800/16000]\t[L1: 3.4571]\t2.6+0.9s\n",
    ">[6400/16000]\t[L1: 3.4436]\t2.6+0.9s\n",
    ">[8000/16000]\t[L1: 3.4697]\t2.6+0.9s\n",
    ">[9600/16000]\t[L1: 3.4793]\t2.6+0.9s\n",
    ">[11200/16000]\t[L1: 3.4532]\t2.6+0.9s\n",
    ">[12800/16000]\t[L1: 3.4586]\t2.6+0.9s\n",
    ">[14400/16000]\t[L1: 3.4524]\t2.6+0.9s\n",
    ">[16000/16000]\t[L1: 3.4467]\t2.6+0.9s\n",
    ">\n",
    ">Evaluation:\n",
    ">[DIV2K x2]\tPSNR: 35.456 (Best: 35.456 @epoch 134)\n",
    ">Forward: 2.15s\n",
    ">\n",
    ">Saving...\n",
    ">Total: 2.45s\n",
    ">```\n",
    "---\n",
    "\n",
    "下面是测试结果，使用了作者给定的最终模型，使用了由AI绘制的图，其中**左图**是经过超分之后的图，右图是原始图片，在不经过放大的情况下，两者很不出肉眼差别。但是左边的图片内存占用为：`12Mb` 右图为：`1.3Mb`\n",
    "<center>\n",
    "<!-- <img src=\"../AI_x4_SR.png\" alt=\"AI_x4\" style=\"zoom:15%;\" /> <img src=\"../AI-org.png\" alt=\"AI_org\" style=\"zoom:40%;\" /> -->\n",
    "    <img src=\"../AI_x4_SR.png\" alt=\"AI_x4\" width=\"400\"  /> <img src=\"../AI-org.png\" alt=\"AI_org\" width=\"400\"  />\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662c3fe4-fe37-441d-afea-5dc88e1651a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
